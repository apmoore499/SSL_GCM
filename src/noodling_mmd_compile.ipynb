{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2153, 0.4306, 0.8611, 1.7222, 3.4444])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd=1.7222\n",
    "\n",
    "slist=[0.125,0.25,0.5,1.0,2.0]\n",
    "\n",
    "\n",
    "sigma_list=torch.tensor([s*pwd for s in slist])\n",
    "\n",
    "sigma_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class mix_rbf_kernel_comp(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,sigma_list):\n",
    "        super(mix_rbf_kernel_comp, self).__init__()\n",
    "        self.sigma_list=sigma_list\n",
    "        \n",
    "        \n",
    "    def forward(self,X,Y):\n",
    "        assert (X.size(0) == Y.size(0))\n",
    "        m = X.size(0)\n",
    "\n",
    "        Z = torch.cat((X, Y), 0)\n",
    "        ZZT = torch.mm(Z, Z.t())\n",
    "        diag_ZZT = torch.diag(ZZT).unsqueeze(1)\n",
    "        Z_norm_sqr = diag_ZZT.expand_as(ZZT)\n",
    "        exponent = Z_norm_sqr - 2 * ZZT + Z_norm_sqr.t()\n",
    "\n",
    "        Klist = torch.zeros(sigma_list.shape[0],exponent.shape[0],exponent.shape[1])\n",
    "\n",
    "\n",
    "\n",
    "        for i,sigma in enumerate(self.sigma_list):\n",
    "            gamma = 1.0 / (2 * sigma ** 2)\n",
    "            Klist[i]= torch.exp(-gamma * exponent)\n",
    "\n",
    "        K=torch.sum(Klist, dim=0)\n",
    "        \n",
    "        return K[:m, :m], K[:m, m:], K[m:, m:], len(self.sigma_list)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class mix_rbf_kernel_comp_moremvec(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,sigma_list):\n",
    "        super(mix_rbf_kernel_comp_moremvec, self).__init__()\n",
    "        self.sigma_list=sigma_list\n",
    "        \n",
    "        \n",
    "        self.placeholder=torch.zeros((2*11000,2)).contiguous()\n",
    "        \n",
    "        \n",
    "    def forward(self,X,Y):\n",
    "        assert (X.size(0) == Y.size(0))\n",
    "        m = X.size(0)\n",
    "        \n",
    "        self.placeholder[:m]=X\n",
    "        self.placeholder[m:m+m]=Y\n",
    "\n",
    "        #Z = torch.cat((X, Y), 0)\n",
    "        ZZT = torch.mm(self.placeholder[:m+m], self.placeholder[:m+m].t())\n",
    "        diag_ZZT = torch.diag(ZZT).unsqueeze(1)\n",
    "        Z_norm_sqr = diag_ZZT.expand_as(ZZT)\n",
    "        exponent = Z_norm_sqr - 2 * ZZT + Z_norm_sqr.t()\n",
    "\n",
    "\n",
    "        K=torch.sum(torch.exp(exponent[None,:,:].expand(self.sigma_list.shape[0],-1,-1) * -(1.0 / (2 * self.sigma_list ** 2))[:,None,None]), dim=0)\n",
    "        \n",
    "        return K[:m, :m], K[:m, m:], K[m:, m:], len(self.sigma_list)\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def _mix_linear_kernel(X, Y):\n",
    "    assert (X.size(0) == Y.size(0))\n",
    "    m = X.size(0)\n",
    "\n",
    "    Z = torch.cat((X, Y), 0)\n",
    "    K = torch.mm(Z, Z.t())\n",
    "\n",
    "    return K[:m, :m], K[:m, m:], K[m:, m:]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "rbb=mix_rbf_kernel_comp(sigma_list=sigma_list)\n",
    "\n",
    "\n",
    "rbb_v=mix_rbf_kernel_comp_morevec(sigma_list=sigma_list)\n",
    "\n",
    "\n",
    "rbb_m=mix_rbf_kernel_comp_moremvec(sigma_list=sigma_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbb_m.placeholder[:10]=X\n",
    "\n",
    "rbb_m.placeholder[10:10+10]=Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbb_v=torch.compile(rbb_v)\n",
    "rbb_c=torch.compile(rbb)\n",
    "\n",
    "rbb_mc=torch.compile(rbb_m,mode='max-autotune')\n",
    "\n",
    "\n",
    "X=torch.randn(10,2)\n",
    "Y=torch.randn(10,2)\n",
    "\n",
    "rbb_vs=torch.compile(rbb_v,mode='max-autotune')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class mix_rbf_kernel_class(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,sigma_list):\n",
    "        super(mix_rbf_kernel_class, self).__init__()\n",
    "        self.sigma_list=sigma_list\n",
    "        \n",
    "\n",
    "    def forward(self,X,Y):\n",
    "        assert (X.size(0) == Y.size(0))\n",
    "        m = X.size(0)\n",
    "        Z = torch.cat((X, Y), 0)\n",
    "        ZZT = torch.mm(Z, Z.t())\n",
    "        diag_ZZT = torch.diag(ZZT).unsqueeze(1)\n",
    "        Z_norm_sqr = diag_ZZT.expand_as(ZZT)\n",
    "        exponent = Z_norm_sqr - 2 * ZZT + Z_norm_sqr.t()\n",
    "        K=torch.sum(torch.exp(exponent[None,:,:].expand(self.sigma_list.shape[0],-1,-1) * -(1.0 / (2 * self.sigma_list ** 2))[:,None,None]), dim=0)\n",
    "        \n",
    "        return K[:m, :m], K[:m, m:], K[m:, m:], len(self.sigma_list)\n",
    "            \n",
    "\n",
    "\n",
    "mix_rbf_mmd2_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "def _mix_linear_kernel(X, Y):\n",
    "    assert (X.size(0) == Y.size(0))\n",
    "    m = X.size(0)\n",
    "\n",
    "    Z = torch.cat((X, Y), 0)\n",
    "    K = torch.mm(Z, Z.t())\n",
    "\n",
    "    return K[:m, :m], K[:m, m:], K[m:, m:]\n",
    "\n",
    "\n",
    "\n",
    "class mix_rbf_mmd2_class(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,sigma_list):\n",
    "        super(mix_rbf_mmd2_class, self).__init__()\n",
    "        self.rbf_kernel=torch.compile(mix_rbf_kernel_class(sigma_list=sigma_list))\n",
    "        \n",
    "    def forward(self,X,Y):\n",
    "        K_XX, K_XY, K_YY, d = self.rbf_kernel(X, Y, sigma_list)\n",
    "        m = K_XX.size(0)    # assume X, Y are same shape\n",
    "\n",
    "        # # Get the various sums of kernels that we'll use\n",
    "        # # Kts drop the diagonal, but we don't need to compute them explicitly\n",
    "        # if const_diagonal is not False:\n",
    "        #     diag_X = diag_Y = const_diagonal\n",
    "        #     sum_diag_X = sum_diag_Y = m * const_diagonal\n",
    "        # else:\n",
    "        diag_X = torch.diag(K_XX)                       # (m,)\n",
    "        diag_Y = torch.diag(K_YY)                       # (m,)\n",
    "        sum_diag_X = torch.sum(diag_X)\n",
    "        sum_diag_Y = torch.sum(diag_Y)\n",
    "\n",
    "        Kt_XX_sums = K_XX.sum(dim=1) - diag_X             # \\tilde{K}_XX * e = K_XX * e - diag_X\n",
    "        Kt_YY_sums = K_YY.sum(dim=1) - diag_Y             # \\tilde{K}_YY * e = K_YY * e - diag_Y\n",
    "        K_XY_sums_0 = K_XY.sum(dim=0)                     # K_{XY}^T * e\n",
    "\n",
    "        Kt_XX_sum = Kt_XX_sums.sum()                       # e^T * \\tilde{K}_XX * e\n",
    "        Kt_YY_sum = Kt_YY_sums.sum()                       # e^T * \\tilde{K}_YY * e\n",
    "        K_XY_sum = K_XY_sums_0.sum()                       # e^T * K_{XY} * e\n",
    "\n",
    "        #if biased:\n",
    "        mmd2 = ((Kt_XX_sum + sum_diag_X) / (m * m)+ (Kt_YY_sum + sum_diag_Y) / (m * m)- 2.0 * K_XY_sum / (m * m))\n",
    "        # else:\n",
    "        #     mmd2 = (Kt_XX_sum / (m * (m - 1))\n",
    "        #         + Kt_YY_sum / (m * (m - 1))\n",
    "        #         - 2.0 * K_XY_sum / (m * m))\n",
    "\n",
    "        return mmd2\n",
    "        \n",
    "        \n",
    "\n",
    "# def mix_rbf_mmd2(X, Y, sigma_list, biased=True):\n",
    "#     # return _mmd2(K_XX, K_XY, K_YY, const_diagonal=d, biased=biased)\n",
    "#     return _mmd2(K_XX, K_XY, K_YY, const_diagonal=False, biased=biased)\n",
    "\n",
    "\n",
    "\n",
    "class mix_rbf_kernel_class(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,sigma_list):\n",
    "        super(mix_rbf_kernel_class, self).__init__()\n",
    "        self.sigma_list=sigma_list\n",
    "        \n",
    "\n",
    "    def forward(self,X,Y):\n",
    "        assert (X.size(0) == Y.size(0))\n",
    "        m = X.size(0)\n",
    "        Z = torch.cat((X, Y), 0)\n",
    "        ZZT = torch.mm(Z, Z.t())\n",
    "        diag_ZZT = torch.diag(ZZT).unsqueeze(1)\n",
    "        Z_norm_sqr = diag_ZZT.expand_as(ZZT)\n",
    "        exponent = Z_norm_sqr - 2 * ZZT + Z_norm_sqr.t()\n",
    "        K=torch.sum(torch.exp(exponent[None,:,:].expand(self.sigma_list.shape[0],-1,-1) * -(1.0 / (2 * self.sigma_list ** 2))[:,None,None]), dim=0)\n",
    "        \n",
    "        return K[:m, :m], K[:m, m:], K[m:, m:], len(self.sigma_list)\n",
    "            \n",
    "\n",
    "class mix_rbf_mmd2_joint_1_feature_1_label(torch.nn.Module):\n",
    "    def __init__(self,sigma_list_effect):\n",
    "        super(mix_rbf_mmd2_joint_1_feature_1_label, self).__init__()\n",
    "        self.rbf_effect=torch.compile(mix_rbf_kernel_class(sigma_list=sigma_list_effect))\n",
    "        self.kern_linear=torch.comiple(_mix_linear_kernel)\n",
    "        \n",
    "        \n",
    "    def forward(self,X_effect_hat,X_effect_true,Y_lab_hat,Y_lab_true):\n",
    "        \n",
    "        \n",
    "        K_XX, K_XY, K_YY, d = self.rbf_effect(X_effect_hat,X_effect_true)\n",
    "        K_XX1, K_XY1, K_YY1 = self.kern_linear(Y_lab_hat,Y_lab_true)\n",
    "        K_XX = K_XX * K_XX1\n",
    "        K_YY = K_YY * K_YY1\n",
    "        K_XY = K_XY * K_XY1\n",
    "        m = K_XX.size(0)    # assume X, Y are same shape\n",
    "\n",
    "        # # Get the various sums of kernels that we'll use\n",
    "        # # Kts drop the diagonal, but we don't need to compute them explicitly\n",
    "        # if const_diagonal is not False:\n",
    "        #     diag_X = diag_Y = const_diagonal\n",
    "        #     sum_diag_X = sum_diag_Y = m * const_diagonal\n",
    "        # else:\n",
    "        diag_X = torch.diag(K_XX)                       # (m,)\n",
    "        diag_Y = torch.diag(K_YY)                       # (m,)\n",
    "        sum_diag_X = torch.sum(diag_X)\n",
    "        sum_diag_Y = torch.sum(diag_Y)\n",
    "\n",
    "        Kt_XX_sums = K_XX.sum(dim=1) - diag_X             # \\tilde{K}_XX * e = K_XX * e - diag_X\n",
    "        Kt_YY_sums = K_YY.sum(dim=1) - diag_Y             # \\tilde{K}_YY * e = K_YY * e - diag_Y\n",
    "        K_XY_sums_0 = K_XY.sum(dim=0)                     # K_{XY}^T * e\n",
    "\n",
    "        Kt_XX_sum = Kt_XX_sums.sum()                       # e^T * \\tilde{K}_XX * e\n",
    "        Kt_YY_sum = Kt_YY_sums.sum()                       # e^T * \\tilde{K}_YY * e\n",
    "        K_XY_sum = K_XY_sums_0.sum()                       # e^T * K_{XY} * e\n",
    "\n",
    "        #if biased:\n",
    "        mmd2 = ((Kt_XX_sum + sum_diag_X) / (m * m)+ (Kt_YY_sum + sum_diag_Y) / (m * m)- 2.0 * K_XY_sum / (m * m))\n",
    "        # else:\n",
    "        #     mmd2 = (Kt_XX_sum / (m * (m - 1))\n",
    "        #         + Kt_YY_sum / (m * (m - 1))\n",
    "        #         - 2.0 * K_XY_sum / (m * m))\n",
    "\n",
    "        return mmd2\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "class mix_rbf_mmd2_joint_regress_2_feature(torch.nn.Module):\n",
    "    def __init__(self,sigma_list_effect,sigma_list_cause):\n",
    "        super(mix_rbf_mmd2_joint_regress_2_feature, self).__init__()\n",
    "        self.rbf_effect=torch.compile(mix_rbf_kernel_class(sigma_list=sigma_list_effect))\n",
    "        self.rbf_cause=torch.compile(mix_rbf_kernel_class(sigma_list=sigma_list_cause))\n",
    "        \n",
    "\n",
    "    def forward(self,X_e_hat,X_e_true,X_c_hat,X_c_true):\n",
    "        K_XX, K_XY, K_YY, d = self.rbf_effect(X_e_hat,X_e_true)\n",
    "        K_XX1, K_XY1, K_YY1, d1 = self.rbf_cause(X_c_hat,X_c_true)\n",
    "        K_XX = K_XX * K_XX1\n",
    "        K_YY = K_YY * K_YY1\n",
    "        K_XY = K_XY * K_XY1\n",
    "\n",
    "        m = K_XX.size(0)    # assume X, Y are same shape\n",
    "\n",
    "        # else:\n",
    "        diag_X = torch.diag(K_XX)                       # (m,)\n",
    "        diag_Y = torch.diag(K_YY)                       # (m,)\n",
    "        sum_diag_X = torch.sum(diag_X)\n",
    "        sum_diag_Y = torch.sum(diag_Y)\n",
    "\n",
    "        Kt_XX_sums = K_XX.sum(dim=1) - diag_X             # \\tilde{K}_XX * e = K_XX * e - diag_X\n",
    "        Kt_YY_sums = K_YY.sum(dim=1) - diag_Y             # \\tilde{K}_YY * e = K_YY * e - diag_Y\n",
    "        K_XY_sums_0 = K_XY.sum(dim=0)                     # K_{XY}^T * e\n",
    "\n",
    "        Kt_XX_sum = Kt_XX_sums.sum()                       # e^T * \\tilde{K}_XX * e\n",
    "        Kt_YY_sum = Kt_YY_sums.sum()                       # e^T * \\tilde{K}_YY * e\n",
    "        K_XY_sum = K_XY_sums_0.sum()                       # e^T * K_{XY} * e\n",
    "\n",
    "        #if biased:\n",
    "        mmd2 = ((Kt_XX_sum + sum_diag_X) / (m * m)+ (Kt_YY_sum + sum_diag_Y) / (m * m)- 2.0 * K_XY_sum / (m * m))\n",
    "        # else:\n",
    "        #     mmd2 = (Kt_XX_sum / (m * (m - 1))\n",
    "        #         + Kt_YY_sum / (m * (m - 1))\n",
    "        #         - 2.0 * K_XY_sum / (m * m))\n",
    "\n",
    "        return mmd2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def mix_rbf_mmd2_joint(X, Y, X1, Y1, X2=None, Y2=None, X3=None, Y3=None, sigma_list=None, biased=True):\n",
    "    K_XX, K_XY, K_YY, d = _mix_rbf_kernel(X, Y, sigma_list)\n",
    "    K_XX1, K_XY1, K_YY1 = _mix_linear_kernel(X1, Y1)\n",
    "    if X2 is None and Y2 is None:\n",
    "        K_XX = K_XX * K_XX1\n",
    "        K_YY = K_YY * K_YY1\n",
    "        K_XY = K_XY * K_XY1\n",
    "    elif X3 is None and Y3 is None:\n",
    "        K_XX2, K_XY2, K_YY2 = _mix_linear_kernel(X2, Y2)\n",
    "        K_XX = K_XX * K_XX1 * K_XX2\n",
    "        K_YY = K_YY * K_YY1 * K_YY2\n",
    "        K_XY = K_XY * K_XY1 * K_XY2\n",
    "    else:\n",
    "        K_XX2, K_XY2, K_YY2 = _mix_linear_kernel(X2, Y2)\n",
    "        K_XX3, K_XY3, K_YY3, d1 = _mix_rbf_kernel(X3, Y3, sigma_list)\n",
    "        K_XX = K_XX * K_XX1 * K_XX2 * K_XX3\n",
    "        K_YY = K_YY * K_YY1 * K_YY2 * K_YY3\n",
    "        K_XY = K_XY * K_XY1 * K_XY2 * K_XY3\n",
    "    # return _mmd2(K_XX, K_XY, K_YY, const_diagonal=d, biased=biased)\n",
    "    return _mmd2(K_XX, K_XY, K_YY, const_diagonal=False, biased=biased)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cudagraphs', 'inductor', 'onnxrt', 'openxla', 'openxla_eval', 'tvm']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch._dynamo.list_backends()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rml=mix_rbf_mmd2_joint_regress_2_feature(sigma_list=sigma_list,sigma_list1=sigma_list)\n",
    "\n",
    "\n",
    "rml_c=torch.compile(rml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def mix_linear_kernel_jit_script(X, Y):\n",
    "    assert (X.size(0) == Y.size(0))\n",
    "    m = X.size(0)\n",
    "\n",
    "    Z = torch.cat((X, Y), 0)\n",
    "    K = torch.mm(Z, Z.t())\n",
    "\n",
    "    return K[:m, :m], K[:m, m:], K[m:, m:]\n",
    "\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def mix_rbf_kernel_jit_script(X,Y,sigma_list):\n",
    "    \n",
    "        assert (X.size(0) == Y.size(0))\n",
    "        m = X.size(0)\n",
    "        Z = torch.cat((X, Y), 0)\n",
    "        ZZT = torch.mm(Z, Z.t())\n",
    "        diag_ZZT = torch.diag(ZZT).unsqueeze(1)\n",
    "        Z_norm_sqr = diag_ZZT.expand_as(ZZT)\n",
    "        exponent = Z_norm_sqr - 2 * ZZT + Z_norm_sqr.t()\n",
    "        K=torch.sum(torch.exp(exponent[None,:,:].expand(5,-1,-1) * -(1.0 / (2 * sigma_list ** 2))[:,None,None]), dim=0)\n",
    "        \n",
    "        return K[:m, :m], K[:m, m:], K[m:, m:], sigma_list.shape[0]\n",
    "            \n",
    "            \n",
    "@torch.jit.script\n",
    "def mix_rbf_mmd2_jit_script(X,Y,sigma_list):\n",
    "    \n",
    "    K_XX, K_XY, K_YY, d = mix_rbf_kernel_jit_script(X, Y,sigma_list)\n",
    "    m = K_XX.size(0)    # assume X, Y are same shape\n",
    "\n",
    "    # # Get the various sums of kernels that we'll use\n",
    "    # # Kts drop the diagonal, but we don't need to compute them explicitly\n",
    "    # if const_diagonal is not False:\n",
    "    #     diag_X = diag_Y = const_diagonal\n",
    "    #     sum_diag_X = sum_diag_Y = m * const_diagonal\n",
    "    # else:\n",
    "    diag_X = torch.diag(K_XX)                       # (m,)\n",
    "    diag_Y = torch.diag(K_YY)                       # (m,)\n",
    "    sum_diag_X = torch.sum(diag_X)\n",
    "    sum_diag_Y = torch.sum(diag_Y)\n",
    "\n",
    "    Kt_XX_sums = K_XX.sum(dim=1) - diag_X             # \\tilde{K}_XX * e = K_XX * e - diag_X\n",
    "    Kt_YY_sums = K_YY.sum(dim=1) - diag_Y             # \\tilde{K}_YY * e = K_YY * e - diag_Y\n",
    "    K_XY_sums_0 = K_XY.sum(dim=0)                     # K_{XY}^T * e\n",
    "\n",
    "    Kt_XX_sum = Kt_XX_sums.sum()                       # e^T * \\tilde{K}_XX * e\n",
    "    Kt_YY_sum = Kt_YY_sums.sum()                       # e^T * \\tilde{K}_YY * e\n",
    "    K_XY_sum = K_XY_sums_0.sum()                       # e^T * K_{XY} * e\n",
    "\n",
    "    #if biased:\n",
    "    mmd2 = ((Kt_XX_sum + sum_diag_X) / (m * m)+ (Kt_YY_sum + sum_diag_Y) / (m * m)- 2.0 * K_XY_sum / (m * m))\n",
    "    # else:\n",
    "    #     mmd2 = (Kt_XX_sum / (m * (m - 1))\n",
    "    #         + Kt_YY_sum / (m * (m - 1))\n",
    "    #         - 2.0 * K_XY_sum / (m * m))\n",
    "\n",
    "    return mmd2\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "@torch.jit.script\n",
    "def mix_rbf_mmd2_joint_1_feature_1_label_jit_script(X_effect_hat,X_effect_true,Y_lab_hat,Y_lab_true,sigma_list_effect):\n",
    "    \n",
    "    K_XX, K_XY, K_YY, d = mix_rbf_kernel_jit_script(X_effect_hat,X_effect_true,sigma_list_effect)\n",
    "    \n",
    "    K_XX1, K_XY1, K_YY1 = mix_linear_kernel_jit_script(Y_lab_hat,Y_lab_true)\n",
    "    \n",
    "    K_XX = K_XX * K_XX1\n",
    "    K_YY = K_YY * K_YY1\n",
    "    K_XY = K_XY * K_XY1\n",
    "    m = K_XX.size(0)    # assume X, Y are same shape\n",
    "\n",
    "    # # Get the various sums of kernels that we'll use\n",
    "    # # Kts drop the diagonal, but we don't need to compute them explicitly\n",
    "    # if const_diagonal is not False:\n",
    "    #     diag_X = diag_Y = const_diagonal\n",
    "    #     sum_diag_X = sum_diag_Y = m * const_diagonal\n",
    "    # else:\n",
    "    diag_X = torch.diag(K_XX)                       # (m,)\n",
    "    diag_Y = torch.diag(K_YY)                       # (m,)\n",
    "    sum_diag_X = torch.sum(diag_X)\n",
    "    sum_diag_Y = torch.sum(diag_Y)\n",
    "\n",
    "    Kt_XX_sums = K_XX.sum(dim=1) - diag_X             # \\tilde{K}_XX * e = K_XX * e - diag_X\n",
    "    Kt_YY_sums = K_YY.sum(dim=1) - diag_Y             # \\tilde{K}_YY * e = K_YY * e - diag_Y\n",
    "    K_XY_sums_0 = K_XY.sum(dim=0)                     # K_{XY}^T * e\n",
    "\n",
    "    Kt_XX_sum = Kt_XX_sums.sum()                       # e^T * \\tilde{K}_XX * e\n",
    "    Kt_YY_sum = Kt_YY_sums.sum()                       # e^T * \\tilde{K}_YY * e\n",
    "    K_XY_sum = K_XY_sums_0.sum()                       # e^T * K_{XY} * e\n",
    "\n",
    "    #if biased:\n",
    "    mmd2 = ((Kt_XX_sum + sum_diag_X) / (m * m)+ (Kt_YY_sum + sum_diag_Y) / (m * m)- 2.0 * K_XY_sum / (m * m))\n",
    "    # else:\n",
    "    #     mmd2 = (Kt_XX_sum / (m * (m - 1))\n",
    "    #         + Kt_YY_sum / (m * (m - 1))\n",
    "    #         - 2.0 * K_XY_sum / (m * m))\n",
    "\n",
    "    return mmd2\n",
    "    \n",
    "    \n",
    "    \n",
    "@torch.jit.script\n",
    "def mix_rbf_mmd2_joint_regress_2_feature_jit_script(X_e_hat,X_e_true,X_c_hat,X_c_true,sigma_list_effect,sigma_list_cause):\n",
    "    K_XX, K_XY, K_YY, d = mix_rbf_kernel_jit_script(X_e_hat,X_e_true,sigma_list_effect)\n",
    "    K_XX1, K_XY1, K_YY1, d1 = mix_rbf_kernel_jit_script(X_c_hat,X_c_true,sigma_list_cause)\n",
    "    K_XX = K_XX * K_XX1\n",
    "    K_YY = K_YY * K_YY1\n",
    "    K_XY = K_XY * K_XY1\n",
    "\n",
    "    m = K_XX.size(0)    # assume X, Y are same shape\n",
    "\n",
    "    # else:\n",
    "    diag_X = torch.diag(K_XX)                       # (m,)\n",
    "    diag_Y = torch.diag(K_YY)                       # (m,)\n",
    "    sum_diag_X = torch.sum(diag_X)\n",
    "    sum_diag_Y = torch.sum(diag_Y)\n",
    "\n",
    "    Kt_XX_sums = K_XX.sum(dim=1) - diag_X             # \\tilde{K}_XX * e = K_XX * e - diag_X\n",
    "    Kt_YY_sums = K_YY.sum(dim=1) - diag_Y             # \\tilde{K}_YY * e = K_YY * e - diag_Y\n",
    "    K_XY_sums_0 = K_XY.sum(dim=0)                     # K_{XY}^T * e\n",
    "\n",
    "    Kt_XX_sum = Kt_XX_sums.sum()                       # e^T * \\tilde{K}_XX * e\n",
    "    Kt_YY_sum = Kt_YY_sums.sum()                       # e^T * \\tilde{K}_YY * e\n",
    "    K_XY_sum = K_XY_sums_0.sum()                       # e^T * K_{XY} * e\n",
    "\n",
    "    #if biased:\n",
    "    mmd2 = ((Kt_XX_sum + sum_diag_X) / (m * m)+ (Kt_YY_sum + sum_diag_Y) / (m * m)- 2.0 * K_XY_sum / (m * m))\n",
    "    # else:\n",
    "    #     mmd2 = (Kt_XX_sum / (m * (m - 1))\n",
    "    #         + Kt_YY_sum / (m * (m - 1))\n",
    "    #         - 2.0 * K_XY_sum / (m * m))\n",
    "\n",
    "    return mmd2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=torch.randn((200,2),dtype=torch.float16,device=torch.device('cuda'))\n",
    "\n",
    "\n",
    "\n",
    "sl=torch.tensor([(2.0)**(k)*1.6666 for k in range(-3,2)],dtype=torch.float16,device=torch.device('cuda'))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#custom compiled kern for if we need to do many mmd per run, speedup\n",
    "\n",
    "def _mix_linear_kernel(X, Y):\n",
    "    assert (X.size(0) == Y.size(0))\n",
    "    m = X.size(0)\n",
    "\n",
    "    Z = torch.cat((X, Y), 0)\n",
    "    K = torch.mm(Z, Z.t())\n",
    "\n",
    "    return K[:m, :m], K[:m, m:], K[m:, m:]\n",
    "\n",
    "\n",
    "\n",
    "class mix_rbf_kernel_class(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,sigma_list):\n",
    "        super(mix_rbf_kernel_class, self).__init__()\n",
    "        self.sigma_list=sigma_list\n",
    "        \n",
    "\n",
    "    def forward(self,X,Y):\n",
    "        assert (X.size(0) == Y.size(0))\n",
    "        m = X.size(0)\n",
    "        Z = torch.cat((X, Y), 0)\n",
    "        ZZT = torch.mm(Z, Z.t())\n",
    "        diag_ZZT = torch.diag(ZZT).unsqueeze(1)\n",
    "        Z_norm_sqr = diag_ZZT.expand_as(ZZT)\n",
    "        exponent = Z_norm_sqr - 2 * ZZT + Z_norm_sqr.t()\n",
    "        K=torch.sum(torch.exp(exponent[None,:,:].expand(self.sigma_list.shape[0],-1,-1) * -(1.0 / (2 * self.sigma_list ** 2))[:,None,None]), dim=0)\n",
    "        \n",
    "        return K[:m, :m], K[:m, m:], K[m:, m:], len(self.sigma_list)\n",
    "            \n",
    "\n",
    "\n",
    "class mix_rbf_mmd2_class(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self,sigma_list):\n",
    "        super(mix_rbf_mmd2_class, self).__init__()\n",
    "        self.rbf_kernel=mix_rbf_kernel_class(sigma_list=sigma_list)\n",
    "        \n",
    "    def forward(self,X,Y):\n",
    "        K_XX, K_XY, K_YY, d = self.rbf_kernel(X, Y)\n",
    "        m = K_XX.size(0)    # assume X, Y are same shape\n",
    "\n",
    "        # # Get the various sums of kernels that we'll use\n",
    "        # # Kts drop the diagonal, but we don't need to compute them explicitly\n",
    "        # if const_diagonal is not False:\n",
    "        #     diag_X = diag_Y = const_diagonal\n",
    "        #     sum_diag_X = sum_diag_Y = m * const_diagonal\n",
    "        # else:\n",
    "        diag_X = torch.diag(K_XX)                       # (m,)\n",
    "        diag_Y = torch.diag(K_YY)                       # (m,)\n",
    "        sum_diag_X = torch.sum(diag_X)\n",
    "        sum_diag_Y = torch.sum(diag_Y)\n",
    "\n",
    "        Kt_XX_sums = K_XX.sum(dim=1) - diag_X             # \\tilde{K}_XX * e = K_XX * e - diag_X\n",
    "        Kt_YY_sums = K_YY.sum(dim=1) - diag_Y             # \\tilde{K}_YY * e = K_YY * e - diag_Y\n",
    "        K_XY_sums_0 = K_XY.sum(dim=0)                     # K_{XY}^T * e\n",
    "\n",
    "        Kt_XX_sum = Kt_XX_sums.sum()                       # e^T * \\tilde{K}_XX * e\n",
    "        Kt_YY_sum = Kt_YY_sums.sum()                       # e^T * \\tilde{K}_YY * e\n",
    "        K_XY_sum = K_XY_sums_0.sum()                       # e^T * K_{XY} * e\n",
    "\n",
    "        #if biased:\n",
    "        mmd2 = ((Kt_XX_sum + sum_diag_X) / (m * m)+ (Kt_YY_sum + sum_diag_Y) / (m * m)- 2.0 * K_XY_sum / (m * m))\n",
    "        # else:\n",
    "        #     mmd2 = (Kt_XX_sum / (m * (m - 1))\n",
    "        #         + Kt_YY_sum / (m * (m - 1))\n",
    "        #         - 2.0 * K_XY_sum / (m * m))\n",
    "\n",
    "        return mmd2\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "class mix_rbf_kernel_class_nosigma(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(mix_rbf_kernel_class_nosigma, self).__init__()\n",
    "        #self.sigma_list=sigma_list\n",
    "        return \n",
    "\n",
    "    def forward(self,X,Y,sigma_list):\n",
    "        assert (X.size(0) == Y.size(0))\n",
    "        m = X.size(0)\n",
    "        Z = torch.cat((X, Y), 0)\n",
    "        ZZT = torch.mm(Z, Z.t())\n",
    "        diag_ZZT = torch.diag(ZZT).unsqueeze(1)\n",
    "        Z_norm_sqr = diag_ZZT.expand_as(ZZT)\n",
    "        exponent = Z_norm_sqr - 2 * ZZT + Z_norm_sqr.t()\n",
    "        K=torch.sum(torch.exp(exponent[None,:,:].expand(5,-1,-1) * -(1.0 / (2 * sigma_list ** 2))[:,None,None]), dim=0)\n",
    "        \n",
    "        return K[:m, :m], K[:m, m:], K[m:, m:], sigma_list.shape[0]\n",
    "            \n",
    "\n",
    "\n",
    "class mix_rbf_mmd2_class_nosigma(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(mix_rbf_mmd2_class_nosigma, self).__init__()\n",
    "        self.rbf_kernel_nosigma=mix_rbf_kernel_class_nosigma()\n",
    "        \n",
    "    def forward(self,X,Y,sigma_list):\n",
    "        K_XX, K_XY, K_YY, d = self.rbf_kernel_nosigma(X, Y,sigma_list)\n",
    "        m = K_XX.size(0)    # assume X, Y are same shape\n",
    "\n",
    "        # # Get the various sums of kernels that we'll use\n",
    "        # # Kts drop the diagonal, but we don't need to compute them explicitly\n",
    "        # if const_diagonal is not False:\n",
    "        #     diag_X = diag_Y = const_diagonal\n",
    "        #     sum_diag_X = sum_diag_Y = m * const_diagonal\n",
    "        # else:\n",
    "        diag_X = torch.diag(K_XX)                       # (m,)\n",
    "        diag_Y = torch.diag(K_YY)                       # (m,)\n",
    "        sum_diag_X = torch.sum(diag_X)\n",
    "        sum_diag_Y = torch.sum(diag_Y)\n",
    "\n",
    "        Kt_XX_sums = K_XX.sum(dim=1) - diag_X             # \\tilde{K}_XX * e = K_XX * e - diag_X\n",
    "        Kt_YY_sums = K_YY.sum(dim=1) - diag_Y             # \\tilde{K}_YY * e = K_YY * e - diag_Y\n",
    "        K_XY_sums_0 = K_XY.sum(dim=0)                     # K_{XY}^T * e\n",
    "\n",
    "        Kt_XX_sum = Kt_XX_sums.sum()                       # e^T * \\tilde{K}_XX * e\n",
    "        Kt_YY_sum = Kt_YY_sums.sum()                       # e^T * \\tilde{K}_YY * e\n",
    "        K_XY_sum = K_XY_sums_0.sum()                       # e^T * K_{XY} * e\n",
    "\n",
    "        #if biased:\n",
    "        mmd2 = ((Kt_XX_sum + sum_diag_X) / (m * m)+ (Kt_YY_sum + sum_diag_Y) / (m * m)- 2.0 * K_XY_sum / (m * m))\n",
    "        # else:\n",
    "        #     mmd2 = (Kt_XX_sum / (m * (m - 1))\n",
    "        #         + Kt_YY_sum / (m * (m - 1))\n",
    "        #         - 2.0 * K_XY_sum / (m * m))\n",
    "\n",
    "        return mmd2\n",
    "        \n",
    "        \n",
    "# def mix_rbf_mmd2(X, Y, sigma_list, biased=True):\n",
    "#     # return _mmd2(K_XX, K_XY, K_YY, const_diagonal=d, biased=biased)\n",
    "#     return _mmd2(K_XX, K_XY, K_YY, const_diagonal=False, biased=biased)\n",
    "\n",
    "\n",
    "class mix_rbf_mmd2_joint_1_feature_1_label(torch.nn.Module):\n",
    "    def __init__(self,sigma_list_effect):\n",
    "        super(mix_rbf_mmd2_joint_1_feature_1_label, self).__init__()\n",
    "        self.rbf_effect=mix_rbf_kernel_class(sigma_list=sigma_list_effect)\n",
    "        self.kern_linear=_mix_linear_kernel\n",
    "        \n",
    "        \n",
    "    def forward(self,X_effect_hat,X_effect_true,Y_lab_hat,Y_lab_true):\n",
    "        \n",
    "        \n",
    "        K_XX, K_XY, K_YY, d = self.rbf_effect(X_effect_hat,X_effect_true)\n",
    "        K_XX1, K_XY1, K_YY1 = self.kern_linear(Y_lab_hat,Y_lab_true)\n",
    "        K_XX = K_XX * K_XX1\n",
    "        K_YY = K_YY * K_YY1\n",
    "        K_XY = K_XY * K_XY1\n",
    "        m = K_XX.size(0)    # assume X, Y are same shape\n",
    "\n",
    "        # # Get the various sums of kernels that we'll use\n",
    "        # # Kts drop the diagonal, but we don't need to compute them explicitly\n",
    "        # if const_diagonal is not False:\n",
    "        #     diag_X = diag_Y = const_diagonal\n",
    "        #     sum_diag_X = sum_diag_Y = m * const_diagonal\n",
    "        # else:\n",
    "        diag_X = torch.diag(K_XX)                       # (m,)\n",
    "        diag_Y = torch.diag(K_YY)                       # (m,)\n",
    "        sum_diag_X = torch.sum(diag_X)\n",
    "        sum_diag_Y = torch.sum(diag_Y)\n",
    "\n",
    "        Kt_XX_sums = K_XX.sum(dim=1) - diag_X             # \\tilde{K}_XX * e = K_XX * e - diag_X\n",
    "        Kt_YY_sums = K_YY.sum(dim=1) - diag_Y             # \\tilde{K}_YY * e = K_YY * e - diag_Y\n",
    "        K_XY_sums_0 = K_XY.sum(dim=0)                     # K_{XY}^T * e\n",
    "\n",
    "        Kt_XX_sum = Kt_XX_sums.sum()                       # e^T * \\tilde{K}_XX * e\n",
    "        Kt_YY_sum = Kt_YY_sums.sum()                       # e^T * \\tilde{K}_YY * e\n",
    "        K_XY_sum = K_XY_sums_0.sum()                       # e^T * K_{XY} * e\n",
    "\n",
    "        #if biased:\n",
    "        mmd2 = ((Kt_XX_sum + sum_diag_X) / (m * m)+ (Kt_YY_sum + sum_diag_Y) / (m * m)- 2.0 * K_XY_sum / (m * m))\n",
    "        # else:\n",
    "        #     mmd2 = (Kt_XX_sum / (m * (m - 1))\n",
    "        #         + Kt_YY_sum / (m * (m - 1))\n",
    "        #         - 2.0 * K_XY_sum / (m * m))\n",
    "\n",
    "        return mmd2\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "class mix_rbf_mmd2_joint_regress_2_feature(torch.nn.Module):\n",
    "    def __init__(self,sigma_list_effect,sigma_list_cause):\n",
    "        super(mix_rbf_mmd2_joint_regress_2_feature, self).__init__()\n",
    "        self.rbf_effect=mix_rbf_kernel_class(sigma_list=sigma_list_effect)\n",
    "        self.rbf_cause=mix_rbf_kernel_class(sigma_list=sigma_list_cause)\n",
    "        \n",
    "\n",
    "    def forward(self,X_e_hat,X_e_true,X_c_hat,X_c_true):\n",
    "        K_XX, K_XY, K_YY, d = self.rbf_effect(X_e_hat,X_e_true)\n",
    "        K_XX1, K_XY1, K_YY1, d1 = self.rbf_cause(X_c_hat,X_c_true)\n",
    "        K_XX = K_XX * K_XX1\n",
    "        K_YY = K_YY * K_YY1\n",
    "        K_XY = K_XY * K_XY1\n",
    "\n",
    "        m = K_XX.size(0)    # assume X, Y are same shape\n",
    "\n",
    "        # else:\n",
    "        diag_X = torch.diag(K_XX)                       # (m,)\n",
    "        diag_Y = torch.diag(K_YY)                       # (m,)\n",
    "        sum_diag_X = torch.sum(diag_X)\n",
    "        sum_diag_Y = torch.sum(diag_Y)\n",
    "\n",
    "        Kt_XX_sums = K_XX.sum(dim=1) - diag_X             # \\tilde{K}_XX * e = K_XX * e - diag_X\n",
    "        Kt_YY_sums = K_YY.sum(dim=1) - diag_Y             # \\tilde{K}_YY * e = K_YY * e - diag_Y\n",
    "        K_XY_sums_0 = K_XY.sum(dim=0)                     # K_{XY}^T * e\n",
    "\n",
    "        Kt_XX_sum = Kt_XX_sums.sum()                       # e^T * \\tilde{K}_XX * e\n",
    "        Kt_YY_sum = Kt_YY_sums.sum()                       # e^T * \\tilde{K}_YY * e\n",
    "        K_XY_sum = K_XY_sums_0.sum()                       # e^T * K_{XY} * e\n",
    "\n",
    "        #if biased:\n",
    "        mmd2 = ((Kt_XX_sum + sum_diag_X) / (m * m)+ (Kt_YY_sum + sum_diag_Y) / (m * m)- 2.0 * K_XY_sum / (m * m))\n",
    "        # else:\n",
    "        #     mmd2 = (Kt_XX_sum / (m * (m - 1))\n",
    "        #         + Kt_YY_sum / (m * (m - 1))\n",
    "        #         - 2.0 * K_XY_sum / (m * m))\n",
    "\n",
    "        return mmd2\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbmm2=torch.compile(mix_rbf_mmd2_class(sigma_list=sl).to(torch.float16).cuda(),mode='max-autotune')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbmm2_ns=torch.compile(mix_rbf_mmd2_class_nosigma().to(torch.float16).cuda(),mode='reduce-overhead')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sigma_list=sl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137 µs ± 924 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "139 µs ± 2.25 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "The slowest run took 1296.76 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "11.2 ms ± 27.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "139 µs ± 224 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "58.1 µs ± 124 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "139 µs ± 349 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit mix_rbf_kernel_jit_script(X,X,sl)\n",
    "\n",
    "\n",
    "%timeit mix_rbf_kernel_jit_script(X,X,sl)\n",
    "\n",
    "\n",
    "\n",
    "%timeit rbmm2(X,X)\n",
    "\n",
    "%timeit mix_rbf_kernel_jit_script(X,X,sl)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 µs ± 219 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "136 µs ± 868 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%timeit rbmm2(X,X)\n",
    "\n",
    "%timeit mix_rbf_kernel_jit_script(X,X,sl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 1681.89 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "16.1 ms ± 39.2 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rbmm2_ns(X,X,sl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.9 µs ± 1.19 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%timeit rbmm2_ns(X,X,sl)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "    def setup_compiled_mmd_losses(self):\n",
    "        \n",
    "        \n",
    "        self.dict_of_mix_rbf_mmd2_joint_regress={}\n",
    "        \n",
    "        for ev in self.conditional_keys: #effect of Y\n",
    "            \n",
    "            self.dict_of_mix_rbf_mmd2_joint_regress[ev]={}\n",
    "            \n",
    "            sigma_list_ev=[self.median_pwd_dict[ev] * i for i in [0.125,0.25,0.5,1,2]] #effect....\n",
    "            \n",
    "            \n",
    "            \n",
    "            self.dict_of_mix_rbf_mmd2_joint_regress[ev]['conditional_y']=torch.compile(mix_rbf_mmd2_joint_1_feature_1_label(sigma_list_effect=sigma_list_ev))\n",
    "            \n",
    "            \n",
    "            for cv in self.unlabelled_keys: #cause of Y\n",
    "                \n",
    "                \n",
    "                sigma_list_cv=[self.median_pwd_dict[cv] * i for i in [0.125,0.25,0.5,1,2]] #cause.....\n",
    "                \n",
    "                compiled_loss=torch.compile(mix_rbf_mmd2_joint_regress_2_feature(sigma_list_effect=sigma_list_ev,sigma_list_cause=sigma_list_cv))\n",
    "                \n",
    "                self.dict_of_mix_rbf_mmd2_joint_regress[ev][cv]=compiled_loss\n",
    "                \n",
    "                # #get effect v\n",
    "                # associated_names = self.dsc.label_names_alphan[ev]\n",
    "                # # retrieve index of feature\n",
    "                # current_feature_idx = [self.feature_idx_subdict[k] for k in associated_names]\n",
    "                # # put into ancestor dict\n",
    "                # current_batch_ground_truth = cur_batch[:, current_feature_idx].view((-1, self.dsc.feature_dim))\n",
    "\n",
    "                # estimate_ev=current_ancestor_dict[ev]\n",
    "                # true_ev=current_batch_ground_truth\n",
    "\n",
    "                # estimate_cv=current_ancestor_dict[cv]\n",
    "                # true_cv=current_ancestor_dict[cv]\n",
    "\n",
    "                # joint_mmd_loss =mix_rbf_mmd2_joint_regress( estimate_ev,\n",
    "                #                                             true_ev,\n",
    "                #                                             estimate_cv,\n",
    "                #                                             true_cv,\n",
    "                #                                             sigma_list=sigma_list_ev,\n",
    "                #                                             sigma_list1=sigma_list_cv)\n",
    "\n",
    "                # joint_feat_mmd_losses.append(joint_mmd_loss)\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X=torch.randn(10,2)\n",
    "Y=torch.randn(10,2)\n",
    "\n",
    "X1=torch.randn(10,2)\n",
    "Y1=torch.randn(10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "192 µs ± 4.91 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n",
      "79.7 µs ± 153 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "190 µs ± 1.42 µs per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "79 µs ± 501 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rml(X,Y,X1,Y1)\n",
    "%timeit rml_c(X,Y,X1,Y1)\n",
    "\n",
    "%timeit rml(X,Y,X1,Y1)\n",
    "%timeit rml_c(X,Y,X1,Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mix_rbf_mmd2_joint(X, Y, X1, Y1, X2=None, Y2=None, X3=None, Y3=None, sigma_list=None, biased=True):\n",
    "    K_XX, K_XY, K_YY, d = _mix_rbf_kernel(X, Y, sigma_list)\n",
    "    K_XX1, K_XY1, K_YY1 = _mix_linear_kernel(X1, Y1)\n",
    "    if X2 is None and Y2 is None:\n",
    "        K_XX = K_XX * K_XX1\n",
    "        K_YY = K_YY * K_YY1\n",
    "        K_XY = K_XY * K_XY1\n",
    "    elif X3 is None and Y3 is None:\n",
    "        K_XX2, K_XY2, K_YY2 = _mix_linear_kernel(X2, Y2)\n",
    "        K_XX = K_XX * K_XX1 * K_XX2\n",
    "        K_YY = K_YY * K_YY1 * K_YY2\n",
    "        K_XY = K_XY * K_XY1 * K_XY2\n",
    "    else:\n",
    "        K_XX2, K_XY2, K_YY2 = _mix_linear_kernel(X2, Y2)\n",
    "        K_XX3, K_XY3, K_YY3, d1 = _mix_rbf_kernel(X3, Y3, sigma_list)\n",
    "        K_XX = K_XX * K_XX1 * K_XX2 * K_XX3\n",
    "        K_YY = K_YY * K_YY1 * K_YY2 * K_YY3\n",
    "        K_XY = K_XY * K_XY1 * K_XY2 * K_XY3\n",
    "    # return _mmd2(K_XX, K_XY, K_YY, const_diagonal=d, biased=biased)\n",
    "    return _mmd2(K_XX, K_XY, K_YY, const_diagonal=False, biased=biased)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/krillman/1TB_DATA/mc3_envs_2/ssl_gcm/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:412: UserWarning: changing options to `torch.compile()` may require calling `torch._dynamo.reset()` to take effect\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.4 µs ± 160 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "46.1 µs ± 173 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skipping cudagraphs for unknown reason\n",
      "/media/krillman/1TB_DATA/mc3_envs_2/ssl_gcm/lib/python3.9/site-packages/torch/_dynamo/eval_frame.py:412: UserWarning: changing options to `torch.compile()` may require calling `torch._dynamo.reset()` to take effect\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77.4 µs ± 27.4 µs per loop (mean ± std. dev. of 7 runs, 1 loop each)\n",
      "45.9 µs ± 98.9 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "46 µs ± 165 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "63 µs ± 217 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%timeit rbb_v(X,Y)\n",
    "\n",
    "\n",
    "%timeit rbb_vs(X,Y)\n",
    "\n",
    "%timeit rbb_mc(X,Y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "%timeit rbb_v(X,Y)\n",
    "\n",
    "\n",
    "%timeit rbb_vs(X,Y)\n",
    "\n",
    "\n",
    "%timeit rbb_mc(X,Y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "%timeit rbb_c(X,Y)\n",
    "#%timeit rbb(X,Y)\n",
    "%timeit rbb_m(X,Y)\n",
    "\n",
    "\n",
    "%timeit rbb_mc(X,Y)\n",
    "\n",
    "%timeit rbb_v(X,Y)\n",
    "\n",
    "%timeit rbb_mc(X,Y)\n",
    "\n",
    "%timeit rbb_v(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 20])\n"
     ]
    }
   ],
   "source": [
    "Z = torch.cat((X, Y), 0)\n",
    "ZZT = torch.mm(Z, Z.t())\n",
    "diag_ZZT = torch.diag(ZZT).unsqueeze(1)\n",
    "Z_norm_sqr = diag_ZZT.expand_as(ZZT)\n",
    "exponent = Z_norm_sqr - 2 * ZZT + Z_norm_sqr.t()\n",
    "\n",
    "\n",
    "print(exponent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 20, 20])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(exponent[None,:,:].expand(sigma_list.shape[0],-1,-1) * -(1.0 / (2 * sigma_list ** 2))[:,None,None]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 20, 20])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exponent[None,:,:].expand(sigma_list.shape[0],-1,-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.7890,  2.6973,  0.6743,  0.1686,  0.0421])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1.0 / (2 * sigma_list ** 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): mix_rbf_kernel_comp()\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbb_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5.0000, 1.5112, 1.6955, 1.1497, 1.0831, 2.8146, 2.3772, 1.1969, 1.8408,\n",
       "          0.7788],\n",
       "         [1.5112, 5.0000, 4.2479, 1.5336, 2.7855, 2.1027, 1.4281, 3.1647, 3.5001,\n",
       "          1.1142],\n",
       "         [1.6955, 4.2479, 5.0000, 1.6258, 2.3603, 2.4335, 1.6185, 2.6346, 3.8791,\n",
       "          1.1461],\n",
       "         [1.1497, 1.5336, 1.6258, 5.0000, 1.2598, 1.5425, 1.6228, 1.2633, 1.3988,\n",
       "          2.5726],\n",
       "         [1.0831, 2.7855, 2.3603, 1.2598, 5.0000, 1.4412, 1.0207, 4.3047, 2.2094,\n",
       "          1.0016],\n",
       "         [2.8146, 2.1027, 2.4335, 1.5425, 1.4412, 5.0000, 2.5392, 1.5852, 2.5159,\n",
       "          1.0357],\n",
       "         [2.3772, 1.4281, 1.6185, 1.6228, 1.0207, 2.5392, 5.0000, 1.0985, 1.5854,\n",
       "          1.1054],\n",
       "         [1.1969, 3.1647, 2.6346, 1.2633, 4.3047, 1.5852, 1.0985, 5.0000, 2.5266,\n",
       "          0.9748],\n",
       "         [1.8408, 3.5001, 3.8791, 1.3988, 2.2094, 2.5159, 1.5854, 2.5266, 5.0000,\n",
       "          0.9860],\n",
       "         [0.7788, 1.1142, 1.1461, 2.5726, 1.0016, 1.0357, 1.1054, 0.9748, 0.9860,\n",
       "          5.0000]]),\n",
       " tensor([[1.5136, 1.8045, 2.7675, 0.9310, 1.4808, 1.6849, 1.0208, 1.1949, 1.5081,\n",
       "          2.3046],\n",
       "         [2.3119, 1.9831, 1.5595, 1.3706, 2.6501, 3.3399, 0.8666, 0.9319, 2.3651,\n",
       "          1.7912],\n",
       "         [2.5284, 2.0170, 1.7754, 1.4136, 2.4859, 3.9906, 0.9500, 1.0297, 2.5827,\n",
       "          2.0467],\n",
       "         [2.6551, 0.9268, 1.5598, 3.3800, 1.0610, 1.9092, 1.7596, 1.7210, 2.6042,\n",
       "          1.8888],\n",
       "         [1.7251, 1.6208, 1.1035, 1.2092, 2.3289, 2.1310, 0.6651, 0.7001, 1.7629,\n",
       "          1.2613],\n",
       "         [2.1837, 1.9144, 2.9984, 1.2523, 1.8027, 2.4987, 1.1568, 1.3164, 2.1804,\n",
       "          3.1389],\n",
       "         [1.8834, 1.2635, 4.2076, 1.2835, 1.1812, 1.7676, 1.6188, 1.9281, 1.8556,\n",
       "          3.4193],\n",
       "         [1.7824, 1.8324, 1.1957, 1.1846, 2.7022, 2.2916, 0.6854, 0.7295, 1.8211,\n",
       "          1.3546],\n",
       "         [2.1137, 2.4567, 1.7712, 1.2104, 2.9412, 3.0207, 0.8644, 0.9518, 2.1473,\n",
       "          1.9547],\n",
       "         [1.6999, 0.6465, 1.0534, 3.4908, 0.7700, 1.3125, 1.5331, 1.3852, 1.6838,\n",
       "          1.2439]]),\n",
       " tensor([[5.0000, 1.3385, 1.9291, 2.1495, 1.5420, 3.1590, 1.3648, 1.4359, 4.9734,\n",
       "          2.4177],\n",
       "         [1.3385, 5.0000, 1.4302, 0.7953, 2.8870, 1.7450, 0.6211, 0.7038, 1.3536,\n",
       "          1.4356],\n",
       "         [1.9291, 1.4302, 5.0000, 1.2395, 1.3183, 1.9056, 1.4337, 1.6872, 1.9069,\n",
       "          3.6495],\n",
       "         [2.1495, 0.7953, 1.2395, 5.0000, 0.9451, 1.6319, 1.5336, 1.4424, 2.1296,\n",
       "          1.4849],\n",
       "         [1.5420, 2.8870, 1.3183, 0.9451, 5.0000, 2.0668, 0.6376, 0.7032, 1.5678,\n",
       "          1.4107],\n",
       "         [3.1590, 1.7450, 1.9056, 1.6319, 2.0668, 5.0000, 1.0872, 1.1695, 3.2407,\n",
       "          2.2812],\n",
       "         [1.3648, 0.6211, 1.4337, 1.5336, 0.6376, 1.0872, 5.0000, 3.6962, 1.3379,\n",
       "          1.5170],\n",
       "         [1.4359, 0.7038, 1.6872, 1.4424, 0.7032, 1.1695, 3.6962, 5.0000, 1.4072,\n",
       "          1.7397],\n",
       "         [4.9734, 1.3536, 1.9069, 2.1296, 1.5678, 3.2407, 1.3379, 1.4072, 5.0000,\n",
       "          2.3826],\n",
       "         [2.4177, 1.4356, 3.6495, 1.4849, 1.4107, 2.2812, 1.5170, 1.7397, 2.3826,\n",
       "          5.0000]]),\n",
       " 5)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "kl=rbb(X,Y)\n",
    "\n",
    "kl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "187 µs ± 614 ns per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rbb(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58.8 µs ± 299 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit rbb_c(X,Y)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.02 ns ± 0.0347 ns per loop (mean ± std. dev. of 7 runs, 100,000,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20, 20])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(kl, dim=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.index_reduce(input=kl,dim=0,index=)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ssl_gcm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
