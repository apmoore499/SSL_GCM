what else needs to be done



- plot the synthetic datasets []
		- gumbel []	
		- disjoint []


- pick n workers for optimal performance: [x] 4 seems to work best

n_workers=8, time taken: 1.8985329866409302 minutes
n_workers=4, time taken: 1.4894595225652059 minutes [x] <- this one is the best
n_workers=16, too slow aborted
n_workers=2, time taken: 1.7493440588315328 minutes
n_workers=1, too slow aborted
n_workers=0, too slow aborted




so if we can get 1.48 minute for every run, that is 148 minutes which is less than 3 hours. for 1 dataset. over all datasets. slow. but the outcome seems to be variable. 



it seems that using:

torch.set_float32_matmul_precision('medium') 

doesn't change accuracy. is slightly higher for 'medium' setting than none. gumbel method with 'medium' is more accurate compared to psup baseline.

```
cgan disjoint, with lr=1e-2, bsize=256,
test_acc: [0.69300002]
unlabel_acc: [0.71222997]
```
we have


cgan disjoint, with lr=1e-3, bsize=4096
but initialise the classifier as per usual

test_acc: [0.60000002]
unlabel_acc: [0.61109]




algo_variant = 'gumbel_disjoint', this one should be used for inference. however it was not synthesised.

plot datasets will give more insight